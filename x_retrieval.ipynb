{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybliometrics.scopus import AbstractRetrieval, AuthorRetrieval, SerialTitle, ScopusSearch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "pd.set_option('max.columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집한 객체에서 각각 필요한 데이터를 리스트로 출력하여 dataframe에 넣을 준비\n",
    "def get_abstract_data(obj):\n",
    "    eid = obj.eid\n",
    "    title = obj.title # Final\n",
    "    abstract = obj.abstract # Final\n",
    "    author_keywords = \"; \".join(obj.authkeywords or \"\") # Final\n",
    "    index_keywords = \"; \".join(obj.idxterms or \"\")\n",
    "    published_year = obj.coverDate[:4] # Final\n",
    "    language = obj.language # 영어 기반의 버트 모델이므로 영어 이외의 문서는 제거\n",
    "    affiliation_ids = \"; \".join([str(affiliation_info.id) for affiliation_info in obj.affiliation or \"\"]) # 소속 기관의 연구자 수, 총 논문 수를 구할 ID\n",
    "    author_ids = \"; \".join([str(author_info.auid) for author_info in obj.authors or \"\"])\n",
    "    journal_issn = obj.issn\n",
    "    refer_ids = \"; \".join([ref.id for ref in obj.references or \"\" if ref.id])\n",
    "    funding_cnt = len(obj.funding or [])\n",
    "\n",
    "    return [eid, title, abstract, author_keywords, index_keywords, published_year, language, affiliation_ids, author_ids, refer_ids, journal_issn, funding_cnt]\n",
    "\n",
    "def get_author_data(auid):\n",
    "    au = AuthorRetrieval(auid)\n",
    "    citaion_count = au.citation_count or 0\n",
    "    cited_by_count = au.cited_by_count or 0\n",
    "    h_index = au.h_index or 0\n",
    "    research_start_year = au.publication_range[0] if au.publication_range else None\n",
    "\n",
    "    return [auid, citaion_count, cited_by_count, h_index, research_start_year]\n",
    "\n",
    "def get_sjr_data(issn):\n",
    "    # issn 데이터 중 동저널 다른 issn을 가진 데이터가 있음 => '00000000 00000000' 띄어쓰기로 구분된 문자열 형태로 자료가 삽입되어 있음\n",
    "    issn_list = issn.split(' ')\n",
    "    for issn in issn_list:\n",
    "        try:\n",
    "            st = SerialTitle(issn, view='ENHANCED', years=\"2004-2010\")\n",
    "            issn_sjr_df = pd.DataFrame(st.sjrlist, columns=['year', 'sjr'])\n",
    "            issn_sjr_df.insert(0, 'issn', issn)\n",
    "            return issn_sjr_df\n",
    "        except Exception as e:\n",
    "            print(issn, '데이터 접근 오류 =>', e)\n",
    "            pass\n",
    "\n",
    "def load_table(table_name, db_file_path='rsc/training_data/X_data.db'):\n",
    "    \"\"\"\n",
    "    데이터베이스 테이블을 가져오는 함수\n",
    "        파라미터\n",
    "            table_name [str] : 테이블 이름\n",
    "            db_file_path [str] : 테이블이 들어있는 데이터베이스 파일 경로\n",
    "        리턴값\n",
    "            table [DataFrame] : 데이터프레임\n",
    "    \"\"\"\n",
    "    query = 'SELECT * FROM {}'.format(table_name)\n",
    "    with sqlite3.connect(db_file_path) as conn:\n",
    "        table = pd.read_sql(query, conn)\n",
    "    \n",
    "    return table\n",
    "\n",
    "def make_query_by_eids(eids:list):\n",
    "    \"\"\"\n",
    "    EID 리스트를 넣으면 ScopusSearch(query=eid( {입력} ))에 맞게 데이터를 가공하여 출력받는 함수\n",
    "    --- 파라미터 ---\n",
    "    eids : 리스트 형태\n",
    "    --- 출력 ---\n",
    "    str : 쿼리\n",
    "    \"\"\"\n",
    "    eid_list = ['\"2-s2.0-' + str(eid) + '\"' for eid in eids]\n",
    "    return \"eid ( \" + \" OR \".join(eid_list) + \" )\"\n",
    "\n",
    "def filter_refer_ids(id_list, filename):\n",
    "    \"\"\"\n",
    "    저장된 References DB와 비교하여 이미 수집한 데이터는 지우고, 수집해야할 데이터만 필터\n",
    "        파라미터\n",
    "        - id_list : 검사할 ID 리스트\n",
    "        - filename : 비교할 DB 파일 경로\n",
    "        리턴\n",
    "        - list : 수집해야할 ID 리스트\n",
    "    \"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "\n",
    "    conn = sqlite3.connect(filename)\n",
    "    df = pd.read_sql(\"select * from refer_table\", conn)\n",
    "    return list(set(id_list) - set(df['eid'])) # id_list 집합에서 DB 파일에 있는 eid 집합을 뺀다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 년도별 저널 SJR 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data 불러와서 수집할 저널 ISSN 정보를 추출\n",
    "conn = sqlite3.connect('rsc/training_data/X_data.db')\n",
    "query = 'SELECT * FROM ABSTRACT_RETRIEVAL_API where language == \"eng\"'\n",
    "X_data = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "ISSNs = X_data.journal_issn.unique()\n",
    "\n",
    "issn_year_sjr_df = pd.DataFrame()\n",
    "\n",
    "for issn in tqdm(ISSNs):\n",
    "    try:\n",
    "        retrieval_df = get_sjr_data(issn)\n",
    "        issn_year_sjr_df = issn_year_sjr_df.append(retrieval_df)\n",
    "    except Exception as e:\n",
    "        print(issn)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('rsc/training_data/X_data.db') as conn:\n",
    "    issn_year_sjr_df.to_sql('JOURNAL_SJR', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논문 서지사항 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X 데이터 수집을 위한 기준 데이터 생성\n",
    "with open(\"rsc/preparation_data/AI_target_eids_2005-2010.txt\", \"r\") as f:\n",
    "     ai_paper_eids = [eid.strip() for eid in f.readlines()]\n",
    "print(\"AI 논문 개수:\", len(ai_paper_eids))\n",
    "\n",
    "with open(\"rsc/preparation_data/automotive_target_eids_2005-2010.txt\", \"r\") as f:\n",
    "     ae_paper_eids = [eid.strip() for eid in f.readlines()]\n",
    "print(\"AE 논문 개수:\", len(ae_paper_eids))\n",
    "\n",
    "df_ai = pd.DataFrame({'eid':ai_paper_eids})\n",
    "df_ai['div'] = 'AI'\n",
    "df_ae = pd.DataFrame({'eid':ae_paper_eids})\n",
    "df_ae['div'] = 'AE'\n",
    "\n",
    "df = pd.concat([df_ai, df_ae]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    conn = sqlite3.connect(\"x_temp_abstract_retrieval.db\")\n",
    "    abstract = pd.read_sql(\"select * from abstract_retrieval\", conn)\n",
    "    conn.close()\n",
    "    eids_for_retrieval = set(df['eid']) - set(abstract['eid'])\n",
    "except:\n",
    "    eids_for_retrieval = df['eid'].tolist()\n",
    "\n",
    "ab_objs = []\n",
    "for eid in tqdm(eids_for_retrieval):\n",
    "    try:\n",
    "        ab = AbstractRetrieval(eid, view='FULL')\n",
    "        ab_objs.append(ab)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_datas = []\n",
    "for ab_obj in tqdm(ab_objs):\n",
    "    abstract_datas.append(get_abstract_data(ab_obj))\n",
    "\n",
    "columns = ['eid', 'title', 'abstract', 'author_keywords', 'index_keywords', 'published_year', 'language', 'affiliation_ids', 'author_ids', 'refer_ids', 'journal_issn', 'funding_cnt'] \n",
    "abstract = pd.DataFrame(abstract_datas, columns=columns)\n",
    "conn = sqlite3.connect(\"rsc/training_data/X_abstract_retrieval.db\")\n",
    "abstract.to_sql('ABSTRACT_RETRIEVAL_API', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저자 관련 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('rsc/training_data/X_data.db')\n",
    "query = 'SELECT * FROM ABSTRACT_RETRIEVAL_API where language == \"eng\"'\n",
    "X_data = pd.read_sql(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저자 관련 데이터 수집 (Author ciations, Author publications, Author H-index) -> 총 3개\n",
    "auids = list(set([auid.strip() for auid in \"; \".join(X_data['author_ids']).split(\"; \") if auid]))\n",
    "\n",
    "# 기존에 수집하여 데이터베이스에 저장한 auid는 제외\n",
    "with sqlite3.connect('rsc/training_data/X_data.db') as conn:\n",
    "    data = pd.read_sql('select * from AUTHOR_RETRIEVAL_API', conn)\n",
    "auids = list(set(auids) - set(data['auid'].astype(str)))\n",
    "\n",
    "print('수집할 저자 관련 데이터 수: {:,}개'.format(len(auids)))\n",
    "\n",
    "for auid in tqdm(auids):\n",
    "    author_data = get_author_data(auid)\n",
    "    with sqlite3.connect('rsc/training_data/X_data.db') as conn:\n",
    "        cursor = conn.cursor()\n",
    "        query = \"insert into AUTHOR_RETRIEVAL_API values (?, ?, ?, ?, ?)\"\n",
    "        cursor.execute(query, author_data)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고문헌 데이터 제목 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_info = load_table(\"ABSTRACT_RETRIEVAL_API\",)\n",
    "\n",
    "# 영어로된 데이터만 추출\n",
    "abstract_info = abstract_info[abstract_info['language'] == 'eng'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4036/8605 [3:22:50<3:42:57,  2.93s/it] "
     ]
    },
    {
     "ename": "Scopus429Error",
     "evalue": "Quota Exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScopus429Error\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49208/1972003340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefer_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_query_by_eids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefer_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScopusSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mrefer_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrefer_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrefer_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\venvs\\venv\\lib\\site-packages\\pybliometrics\\scopus\\scopus_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, query, refresh, view, verbose, download, integrity_fields, integrity_action, subscriber, **kwds)\u001b[0m\n\u001b[0;32m    196\u001b[0m         Search.__init__(self, query=query, api='ScopusSearch', count=count,\n\u001b[0;32m    197\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubscriber\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                         verbose=verbose, **kwds)\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\venvs\\venv\\lib\\site-packages\\pybliometrics\\scopus\\superclasses\\search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, query, api, count, cursor, download, verbose, **kwds)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         Base.__init__(self, params=params, url=URLS[api], download=download,\n\u001b[1;32m---> 62\u001b[1;33m                       api=api, verbose=verbose)\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_results_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\venvs\\venv\\lib\\site-packages\\pybliometrics\\scopus\\superclasses\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, url, api, download, verbose, *args, **kwds)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msearch_request\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\venvs\\venv\\lib\\site-packages\\pybliometrics\\scopus\\utils\\get_content.py\u001b[0m in \u001b[0;36mget_content\u001b[1;34m(url, api, params, **kwds)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mScopus429Error\u001b[0m: Quota Exceeded"
     ]
    }
   ],
   "source": [
    "# 학습 데이터의 참고문헌 제목을 사용하기 위해 데이터 수집\n",
    "DB_FILE_PATH = \"./rsc/training_data/reference.db\"\n",
    "\n",
    "refer_ids = [refer_id.strip() for refer_id in \"; \".join(abstract_info['refer_ids']).split(\"; \") if refer_id != \"\"]\n",
    "try:\n",
    "    refer_ids = filter_refer_ids(refer_ids, DB_FILE_PATH)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in tqdm(range((len(refer_ids)//100))):\n",
    "    query = make_query_by_eids(refer_ids[100*i:100*(i+1)])\n",
    "    s = ScopusSearch(query=query, download=True)\n",
    "    refer_df = pd.DataFrame(s.results)\n",
    "    refer_df = refer_df[['eid', 'title']]\n",
    "\n",
    "    conn = sqlite3.connect(DB_FILE_PATH)\n",
    "    refer_df.to_sql(\"refer_table\", conn, if_exists='append', index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "### 테이블명\n",
    "- ABSTRACT_RETRIEVAL_API\n",
    "- AUTHOR_RETRIEVAL_API\n",
    "- JOURNAL_SJR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL로 데이터 불러오기\n",
    "journal_sjr = load_table(\"JOURNAL_SJR\")\n",
    "journal_sjr.drop_duplicates(inplace=True) # 중복 제거\n",
    "author_info = load_table(\"AUTHOR_RETRIEVAL_API\")\n",
    "abstract_info = load_table(\"ABSTRACT_RETRIEVAL_API\")\n",
    "\n",
    "# 영어로된 데이터만 추출\n",
    "abstract_info = abstract_info[abstract_info['language'] == 'eng'].reset_index(drop=True)\n",
    "\n",
    "# Journal_ISSN이 스페이스를 구분으로 2개씩 붙어있는 것들이 있으므로 컬럼을 2개로 나눈다.\n",
    "abstract_info = pd.concat([abstract_info, abstract_info['journal_issn'].str.split(\" \", expand=True).rename(columns={0:\"issn1\", 1:\"issn2\"})], axis=1)\n",
    "\n",
    "\n",
    "# AI, AE 라벨 붙이기 \n",
    "eid_label_dict = {}\n",
    "\n",
    "with open(\"./rsc/preparation_data/automotive_target_eids_2005-2010.txt\", 'r') as f:\n",
    "    ae_eids = f.readlines()\n",
    "ae_eids = {ae_eid.strip():'AE' for ae_eid in ae_eids}\n",
    "\n",
    "with open(\"./rsc/preparation_data/AI_target_eids_2005-2010.txt\", 'r') as f:\n",
    "    ai_eids = f.readlines()\n",
    "ai_eids = {ai_eid.strip():'AI' for ai_eid in ai_eids}\n",
    "\n",
    "eid_label_dict.update(ae_eids)\n",
    "eid_label_dict.update(ai_eids)\n",
    "\n",
    "abstract_info.insert(1, 'field', abstract_info['eid'].map(eid_label_dict))\n",
    "\n",
    "##### (4) Journal Impact Factor(JIF) #####\n",
    "# JOIN 하기 위해 데이터의 타입과 컬럼명 변경 작업 진행\n",
    "abstract_info = abstract_info.astype(str)\n",
    "journal_sjr = journal_sjr.astype(str)\n",
    "\n",
    "# 데이터 합치기\n",
    "x_temp = pd.merge(abstract_info, journal_sjr, how='left', left_on=['issn1', 'published_year'], right_on=['issn', 'year'])\n",
    "abstract_info = pd.merge(x_temp, journal_sjr, how='left', left_on=['issn2', 'published_year'], right_on=['issn', 'year'])\n",
    "\n",
    "# 데이터 합친 후 필요 없는 컬럼 제거\n",
    "abstract_info = abstract_info.drop(labels=['journal_issn', 'issn_x', 'year_x', 'issn_y', 'year_y'], axis=1)\n",
    "\n",
    "# Bibliometric factor 가공\n",
    "abstract_info['sjr_x'] = abstract_info['sjr_x'].fillna(\"\")\n",
    "abstract_info['sjr_x'] = abstract_info.apply(lambda x: x['sjr_x'] if x['sjr_x'] else x['sjr_y'], axis=1)\n",
    "abstract_info = abstract_info.drop(labels=['sjr_y'], axis=1).rename(columns={'sjr_x':'sjr'})\n",
    "###########################################\n",
    "\n",
    "##### (6) 저자 수  #####\n",
    "abstract_info['number_of_authors'] = abstract_info['author_ids'].str.split(\";\").str.len()\n",
    "###########################################\n",
    "\n",
    "##### (7) Number of institutions #####\n",
    "abstract_info['number_of_institutions'] = abstract_info['affiliation_ids'].str.split(\";\").str.len()\n",
    "###########################################\n",
    "\n",
    "##### (8) Number of references #####\n",
    "abstract_info['number_of_references'] = abstract_info['refer_ids'].str.split(\";\").str.len()\n",
    "###########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstract_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/xglnp36d6jl1w3568_cdr5zm0000gn/T/ipykernel_79163/1865919104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabstract_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abstract_info' is not defined"
     ]
    }
   ],
   "source": [
    "abstract_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 사전학습 언어모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint, extract_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./engi_bert_L-12_H-768_A-12/bert_config.json\"\n",
    "ckpt_path = \"./engi_bert_L-12_H-768_A-12/bert_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    ckpt_path,\n",
    "    training=False,\n",
    "    trainable=False,\n",
    "    seq_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import extract_embeddings\n",
    "\n",
    "model_path = 'engi_bert_L-12_H-768_A-12'\n",
    "texts = [\n",
    "    ('automotive is a car.'),\n",
    "    ('A car i automotive.'),\n",
    "]\n",
    "\n",
    "embeddings = extract_embeddings(model_path, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with open('./engi_bert_L-12_H-768_A-12/vocab.txt', 'r') as f:\n",
    "    tokens = f.readlines()\n",
    "\n",
    "token_list = [token.strip() for token in tokens]\n",
    "\n",
    "for idx, value in enumerate(token_list):\n",
    "    token_dict[value] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "tokenizer.tokenize(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFAutoModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./engi_bert_L-12_H-768_A-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('automotive is a car.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hiromoon166/load-bert-fine-tuning-model 참고\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "config_file = \"./engi_bert_L-12_H-768_A-12/bert_config.json\"\n",
    "ckpt_file = \"./engi_bert_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "\n",
    "model = load_trained_model_from_checkpoint(config_file, ckpt_file, training=False, seq_len=512)\n",
    "# model.summary()\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba2ebf9e903956729814961c18dd36989a9e57d3ded303e2564c55c562dfd276"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('myvenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
